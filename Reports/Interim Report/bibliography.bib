@article{bernstein_protein_1977,
	title = {The {Protein} {Data} {Bank}: a computer-based archival file for macromolecular structures},
	volume = {112},
	issn = {0022-2836},
	shorttitle = {The {Protein} {Data} {Bank}},
	doi = {10.1016/s0022-2836(77)80200-3},
	language = {eng},
	number = {3},
	journal = {Journal of Molecular Biology},
	author = {Bernstein, F. C. and Koetzle, T. F. and Williams, G. J. and Meyer, E. F. and Brice, M. D. and Rodgers, J. R. and Kennard, O. and Shimanouchi, T. and Tasumi, M.},
	month = may,
	year = {1977},
	pmid = {875032},
	keywords = {Computers, Information Systems, Japan, Protein Conformation, Proteins, United Kingdom, United States},
	pages = {535--542},
}
@book{lam_hadoop_2010,
	title = {Hadoop in {Action}},
	isbn = {978-1-63835-210-5},
	abstract = {Hadoop in Action teaches readers how to use Hadoop and write MapReduce programs. The intended readers are programmers, architects, and project managers who have to process large amounts of data offline. Hadoop in Action will lead the reader from obtaining a copy of Hadoop to setting it up in a cluster and writing data analytic programs.The book begins by making the basic idea of Hadoop and MapReduce easier to grasp by applying the default Hadoop installation to a few easy-to-follow tasks, such as analyzing changes in word frequency across a body of documents. The book continues through the basic concepts of MapReduce applications developed using Hadoop, including a close look at framework components, use of Hadoop for a variety of data analysis tasks, and numerous examples of Hadoop in action.Hadoop in Action will explain how to use Hadoop and present design patterns and practices of programming MapReduce. MapReduce is a complex idea both conceptually and in its implementation, and Hadoop users are challenged to learn all the knobs and levers for running Hadoop. This book takes you beyond the mechanics of running Hadoop, teaching you to write meaningful programs in a MapReduce framework.This book assumes the reader will have a basic familiarity with Java, as most code examples will be written in Java. Familiarity with basic statistical concepts (e.g. histogram, correlation) will help the reader appreciate the more advanced data processing examples. Purchase of the print book comes with an offer of a free PDF, ePub, and Kindle eBook from Manning. Also available is all code from the book.},
	language = {en},
	publisher = {Simon and Schuster},
	author = {Lam, Chuck},
	month = nov,
	year = {2010},
	note = {Google-Books-ID: 8DozEAAAQBAJ},
	keywords = {Computers / Languages / Java},
}
@inproceedings{shafer_hadoop_2010,
	title = {The {Hadoop} distributed filesystem: {Balancing} portability and performance},
	shorttitle = {The {Hadoop} distributed filesystem},
	doi = {10.1109/ISPASS.2010.5452045},
	abstract = {Hadoop is a popular open-source implementation of MapReduce for the analysis of large datasets. To manage storage resources across the cluster, Hadoop uses a distributed user-level filesystem. This filesystem - HDFS - is written in Java and designed for portability across heterogeneous hardware and software platforms. This paper analyzes the performance of HDFS and uncovers several performance issues. First, architectural bottlenecks exist in the Hadoop implementation that result in inefficient HDFS usage due to delays in scheduling new MapReduce tasks. Second, portability limitations prevent the Java implementation from exploiting features of the native platform. Third, HDFS implicitly makes portability assumptions about how the native platform manages storage resources, even though native filesystems and I/O schedulers vary widely in design and behavior. This paper investigates the root causes of these performance bottlenecks in order to evaluate tradeoffs between portability and performance in the Hadoop distributed filesystem.},
	booktitle = {2010 {IEEE} {International} {Symposium} on {Performance} {Analysis} of {Systems} \& {Software} ({ISPASS})},
	author = {Shafer, Jeffrey and Rixner, Scott and Cox, Alan L.},
	month = mar,
	year = {2010},
	keywords = {Application software, Data analysis, Databases, Delay, Hardware, Java, Open source software, Performance analysis, Processor scheduling, Resource management},
	pages = {122--133},
	file = {IEEE Xplore Abstract Record:/Users/vinaykakkar/Zotero/storage/WZ6ZLP84/5452045.html:text/html;IEEE Xplore Full Text PDF:/Users/vinaykakkar/Zotero/storage/BWE9ZYIW/Shafer et al. - 2010 - The Hadoop distributed filesystem Balancing porta.pdf:application/pdf},
}
@article{meng_mllib_2016,
	title = {{MLlib}: {Machine} {Learning} in {Apache} {Spark}},
	volume = {17},
	issn = {1533-7928},
	shorttitle = {{MLlib}},
	url = {http://jmlr.org/papers/v17/15-237.html},
	abstract = {Apache Spark is a popular open-source platform for large-scale data processing that is well-suited for iterative machine learning tasks. In this paper we present MLlib, Spark's open- source distributed machine learning library. MLlib provides efficient functionality for a wide range of learning settings and includes several underlying statistical, optimization, and linear algebra primitives. Shipped with Spark, MLlib supports several languages and provides a high-level API that leverages Spark's rich ecosystem to simplify the development of end-to-end machine learning pipelines. MLlib has experienced a rapid growth due to its vibrant open-source community of over 140 contributors, and includes extensive documentation to support further growth and to let users quickly get up to speed.},
	number = {34},
	urldate = {2022-12-05},
	journal = {Journal of Machine Learning Research},
	author = {Meng, Xiangrui and Bradley, Joseph and Yavuz, Burak and Sparks, Evan and Venkataraman, Shivaram and Liu, Davies and Freeman, Jeremy and Tsai, D. B. and Amde, Manish and Owen, Sean and Xin, Doris and Xin, Reynold and Franklin, Michael J. and Zadeh, Reza and Zaharia, Matei and Talwalkar, Ameet},
	year = {2016},
	pages = {1--7},
	file = {Full Text PDF:/Users/vinaykakkar/Zotero/storage/JGEYARMH/Meng et al. - 2016 - MLlib Machine Learning in Apache Spark.pdf:application/pdf;Source Code:/Users/vinaykakkar/Zotero/storage/KJ4RTXVA/downloads.html:text/html},
}
@inproceedings{hazarika_performance_2017,
	title = {Performance comparision of {Hadoop} and spark engine},
	doi = {10.1109/I-SMAC.2017.8058263},
	abstract = {Data has been growing at an exponential rate in the recent era. This data has to be processed and analyzed carefully to get new insights. So there is a need for a platform that can perform efficient data processing, which leads to the building up of platforms like Hadoop and Spark. Hadoop Map Reduce model helped to process it in a distributed fashion with computation being done at multiple nodes. In spite of the remarkable processing power it still, has some shortcomings. Spark, a computation engine can solve some of the problems involving Iterative/Machine learning queries by caching some of the results from previous queries. Although Spark is faster than Hadoop in most of the iterative applications it is constrained by memory requirements. This paper briefly discusses Spark and Hadoop architecture, their theoretical differences and the comparison of their performance.},
	booktitle = {2017 {International} {Conference} on {I}-{SMAC} ({IoT} in {Social}, {Mobile}, {Analytics} and {Cloud}) ({I}-{SMAC})},
	author = {Hazarika, Akaash Vishal and Ram, G Jagadeesh Sai Raghu and Jain, Eeti},
	month = feb,
	year = {2017},
	keywords = {Big Data, Computer architecture, Directed Acyclic graphs, Engines, Fault tolerance, Fault tolerant systems, Hadoop, Lazy Evaluation, Logistics, MapReduce, Mobile communication, Resilient Distributed Datasets, Spark, Sparks},
	pages = {671--674},
}